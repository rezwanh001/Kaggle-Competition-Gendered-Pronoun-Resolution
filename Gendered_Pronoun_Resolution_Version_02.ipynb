{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gendered Pronoun Resolution: Version-02.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezwanh001/Kaggle-Competition-Gendered-Pronoun-Resolution/blob/master/Gendered_Pronoun_Resolution_Version_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "OaHnm9HUab0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3cfda184-7f77-43a2-a2cc-bf7fe9d610cc"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kqeNH_3Raktz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import pprint\n",
        "# import tensorflow as tf\n",
        "\n",
        "# if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "#   print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "# else:\n",
        "#   tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "#   print ('TPU address is', tpu_address)\n",
        "\n",
        "#   with tf.Session(tpu_address) as session:\n",
        "#     devices = session.list_devices()\n",
        "    \n",
        "#   print('TPU devices:')\n",
        "#   pprint.pprint(devices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xWirk_Vga7sj",
        "colab_type": "code",
        "outputId": "c14c4a62-5148-4553-87ca-e125466772b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TnCcxAAmbEUV",
        "colab_type": "code",
        "outputId": "709539e4-25ba-4c60-c803-e32a38ede429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "cell_type": "code",
      "source": [
        "# to access kaggle datasets\n",
        "!pip install kaggle\n",
        "\n",
        "# Math operations\n",
        "!pip install numpy==1.15.0\n",
        "\n",
        "#\n",
        "!pip install https://github.com/fchollet/keras/archive/cudnn.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.2)\n",
            "Requirement already satisfied: urllib3<1.23.0,>=1.15 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.11.29)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.0.1)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: Unidecode>=0.04.16 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.0.23)\n",
            "Requirement already satisfied: numpy==1.15.0 in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
            "Collecting https://github.com/fchollet/keras/archive/cudnn.zip\n",
            "\u001b[31m  HTTP error 404 while getting https://github.com/fchollet/keras/archive/cudnn.zip\u001b[0m\n",
            "\u001b[31m  Could not install requirement https://github.com/fchollet/keras/archive/cudnn.zip because of error 404 Client Error: Not Found for url: https://codeload.github.com/keras-team/keras/zip/cudnn\u001b[0m\n",
            "\u001b[31mCould not install requirement https://github.com/fchollet/keras/archive/cudnn.zip because of HTTP error 404 Client Error: Not Found for url: https://codeload.github.com/keras-team/keras/zip/cudnn for URL https://github.com/fchollet/keras/archive/cudnn.zip\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VAxLPHMpbJlJ",
        "colab_type": "code",
        "outputId": "4270fa27-2b47-44c3-a36e-9e908b17a9f8",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "# Colab's file access feature\n",
        "from google.colab import files\n",
        "\n",
        "# retrieve upload file\n",
        "uploaded = files.upload()\n",
        "\n",
        "#print results\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# Then move kaggle.jason into the folder where the API expects to to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/kaggle/kaggle.json "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-17788ef8-c743-4e87-bb13-4dcd2cebf38c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-17788ef8-c743-4e87-bb13-4dcd2cebf38c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 65 bytes\n",
            "chmod: cannot access '/root/kaggle/kaggle.json': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_c-nSdN6bNKM",
        "colab_type": "code",
        "outputId": "771b9e05-dd39-427f-ae0e-badea5653c7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "# list competitions\n",
        "!kaggle competitions list\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "ref                                            deadline             category            reward  teamCount  userHasEntered  \n",
            "---------------------------------------------  -------------------  ---------------  ---------  ---------  --------------  \n",
            "digit-recognizer                               2030-01-01 00:00:00  Getting Started  Knowledge       2499           False  \n",
            "titanic                                        2030-01-01 00:00:00  Getting Started  Knowledge       9876            True  \n",
            "house-prices-advanced-regression-techniques    2030-01-01 00:00:00  Getting Started  Knowledge       4079           False  \n",
            "imagenet-object-localization-challenge         2029-12-31 07:00:00  Research         Knowledge         35           False  \n",
            "competitive-data-science-predict-future-sales  2019-12-31 23:59:00  Playground           Kudos       2384           False  \n",
            "two-sigma-financial-news                       2019-07-15 23:59:00  Featured          $100,000       2927           False  \n",
            "LANL-Earthquake-Prediction                     2019-06-03 23:59:00  Research           $50,000       1278            True  \n",
            "tmdb-box-office-prediction                     2019-05-30 23:59:00  Playground       Knowledge        255           False  \n",
            "dont-overfit-ii                                2019-05-07 23:59:00  Playground            Swag        715           False  \n",
            "gendered-pronoun-resolution                    2019-04-22 23:59:00  Research           $25,000        238            True  \n",
            "santander-customer-transaction-prediction      2019-04-10 23:59:00  Featured           $65,000       2015           False  \n",
            "womens-machine-learning-competition-2019       2019-04-09 23:59:00  Featured           $25,000         83           False  \n",
            "mens-machine-learning-competition-2019         2019-04-08 23:59:00  Featured           $25,000        133           False  \n",
            "histopathologic-cancer-detection               2019-03-30 23:59:00  Playground       Knowledge        697           False  \n",
            "petfinder-adoption-prediction                  2019-03-28 23:59:00  Featured           $25,000       1277           False  \n",
            "vsb-power-line-fault-detection                 2019-03-21 23:59:00  Featured           $25,000       1024           False  \n",
            "microsoft-malware-prediction                   2019-03-13 23:59:00  Research           $25,000       1877           False  \n",
            "humpback-whale-identification                  2019-02-28 23:59:00  Featured           $25,000       2055           False  \n",
            "elo-merchant-category-recommendation           2019-02-26 23:59:00  Featured           $50,000       4105            True  \n",
            "ga-customer-revenue-prediction                 2019-02-21 20:04:00  Featured           $45,000       1100            True  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rQwJfvLnbUp-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "bd451e6f-65e5-460a-e6d9-165548aa74ae"
      },
      "cell_type": "code",
      "source": [
        "#download gendered-pronoun-resolution data\n",
        "!kaggle competitions download -c gendered-pronoun-resolution"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading sample_submission_stage_1.csv to /content\n",
            "  0% 0.00/79.0k [00:00<?, ?B/s]\n",
            "100% 79.0k/79.0k [00:00<00:00, 69.5MB/s]\n",
            "Downloading test_stage_1.tsv.zip to /content\n",
            "  0% 0.00/425k [00:00<?, ?B/s]\n",
            "100% 425k/425k [00:00<00:00, 60.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RS-qcHXgf96P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d4210fb4-7583-4fff-90c5-92494d340d1b"
      },
      "cell_type": "code",
      "source": [
        "###=============== Import necessary Libraries ==================\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import string\n",
        "import keras\n",
        "from pandas.io.json import json_normalize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "from math import floor\n",
        "import spacy\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from plotly import tools\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import lightgbm as lgb\n",
        "\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, CuDNNGRU, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, Concatenate, Add, Flatten, CuDNNLSTM\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.engine.topology import Layer\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np # linear algebra\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "pd.options.display.max_columns = 999\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# keras libraries\n",
        "from keras.models import Model, load_model,Sequential\n",
        "from keras.layers import Dense, Input, Dropout,Bidirectional, GRU, Activation, concatenate, Embedding, SpatialDropout1D\n",
        "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D ,GlobalMaxPool1D, GlobalAvgPool1D, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras import layers\n",
        "\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/vnd.plotly.v1+html": "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>",
            "text/html": [
              "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "viX7O-jtgaVy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "e199d8d8-08f8-4cc5-890b-1defb4407a70"
      },
      "cell_type": "code",
      "source": [
        "##======== Load text and tokenize ====================\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'A few days later, Abigail complained that Elizabeth was pinching her and tearing at her bowels')\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_, token.dep_)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A DET DT det\n",
            "few ADJ JJ amod\n",
            "days NOUN NNS npadvmod\n",
            "later ADV RB advmod\n",
            ", PUNCT , punct\n",
            "Abigail PROPN NNP nsubj\n",
            "complained VERB VBD ROOT\n",
            "that ADP IN mark\n",
            "Elizabeth PROPN NNP nsubj\n",
            "was VERB VBD aux\n",
            "pinching VERB VBG ccomp\n",
            "her PRON PRP dobj\n",
            "and CCONJ CC cc\n",
            "tearing VERB VBG conj\n",
            "at ADP IN prep\n",
            "her ADJ PRP$ poss\n",
            "bowels NOUN NNS pobj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "feEuY8aQg3LE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "####======================= Define necessary libraries ========================\n",
        "def word_locate(sentence, location): \n",
        "    count_words = 0\n",
        "    count_chars = 2 #2 is to count for the two spaces in the beginning\n",
        "    for word in sentence.split():\n",
        "        count_words += 1\n",
        "        if location == count_chars:\n",
        "            return word, count_words\n",
        "        count_chars += len(word)\n",
        "        count_chars += 1 #for space\n",
        "        \n",
        "def curr_prev_sentence(sentence, loc):\n",
        "    current_sentence = \"\"\n",
        "    prev_sentence = \"\"\n",
        "    detect = 0\n",
        "    count = 0\n",
        "    for char in sentence:\n",
        "        count += 1\n",
        "        current_sentence += char\n",
        "        if char == \".\" and detect == 0:\n",
        "            prev_sentence = current_sentence \n",
        "            current_sentence = \"\"\n",
        "        if char == \".\" and detect == 1:\n",
        "            return current_sentence, prev_sentence\n",
        "        if count == loc:\n",
        "            detect = 1\n",
        "\n",
        "def find_subject(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"nsubj\":\n",
        "            return token.text\n",
        "    return \"none\"            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k9x3mnHahn0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "32648397-1e1f-4845-c86e-e93e6d27b6df"
      },
      "cell_type": "code",
      "source": [
        "text1 = \"After a few years of almost no work -- although he was a guest star on Lou Grant and Charlie's Angels in the late 1970s, he once summed up the 1970s as 'I cried and did a lot of gardening' -- he was hired in 1979 for his best-known role, self-made millionaire Palmer Cortlandt on ABC's long-running soap opera All My Children. Initially hired for only one year, he remained on contract through 2009. For much of his first decade on the show, Palmer was a ruthless villain, totally possessive of his daughter, Nina and violently threatening his ex-wife Daisy with being attacked by dobermans when she came back from the dead.\"\n",
        "\n",
        "text2 = \"When onlookers expressed doubt, claiming that the Proctor family was well regarded in the community, the girl promptly came out of her trance and told them it was all for 'sport'. On March 29, 1692, Abigail Williams and Mercy Lewis again said they were being tormented by Elizabeth's spectre. A few days later, Abigail complained that Elizabeth was pinching her and tearing at her bowels, and said she saw Elizabeth's spectre as well as John's.\"\n",
        "\n",
        "current, prev = curr_prev_sentence(text2, 360)\n",
        "print(current)\n",
        "\n",
        "candidate = find_subject(current)\n",
        "print(candidate)\n",
        "\n",
        "word, loc = word_locate(text2, 360) \n",
        "print(word, \" \", loc)\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " A few days later, Abigail complained that Elizabeth was pinching her and tearing at her bowels, and said she saw Elizabeth's spectre as well as John's.\n",
            "Abigail\n",
            "her   60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bYPmwZvbh4hv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ca61ab71-1e1e-48db-a404-0e102e3435ca"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t\t sample_submission_stage_1.csv\ttest_stage_1.tsv.zip\n",
            "gap-coreference  submission_1.csv\n",
            "sample_data\t test_stage_1.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nzK3Yj7YiVIQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6d07e3c5-68da-40f4-9ef3-bd83a0b4ec6c"
      },
      "cell_type": "code",
      "source": [
        "!unzip test_stage_1.tsv.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  test_stage_1.tsv.zip\n",
            "  inflating: test_stage_1.tsv        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y84y7wFuicdD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "6d9bfbcb-0959-4be7-f3db-270232337caf"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "!git clone https://github.com/google-research-datasets/gap-coreference.git\n",
        "\n",
        "#!wget 'https://github.com/google-research-datasets/gap-coreference/blob/master/gap-development.tsv'\n",
        "\n",
        "#!files.download('https://github.com/google-research-datasets/gap-coreference/blob/master/gap-development.tsv') # then browse, select the files. It's then uploaded\n",
        "\n",
        "# uploaded is now a dict containing \"filename\" -> Content"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gap-coreference'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Total 19 (delta 0), reused 0 (delta 0), pack-reused 19\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OSj7ykj9kmPp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "76b53ab5-0eaa-4a8e-f739-ed3e2b4ca39b"
      },
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t\t sample_submission_stage_1.csv\ttest_stage_1.tsv.zip\n",
            "gap-coreference  submission_1.csv\n",
            "sample_data\t test_stage_1.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RRXhMeXYlBFv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### ======================== load test_stage_1\n",
        "with open('test_stage_1.tsv') as tsvfile:\n",
        "    reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
        "    testA_X = []\n",
        "    testB_X = []\n",
        "    test_auX = []\n",
        "    test_ids = []\n",
        "    train_auX = [] #we do a small hack and fill up the aux train data twice, like train hopping \n",
        "    for row in reader:\n",
        "        text = row['Text']\n",
        "        textA = text.lower()\n",
        "        test_auX.append(text.lower())#test aux is half size of train aux\n",
        "        train_auX.append(text.lower()) #since test loop is running, why not fill up aux train in meantime \n",
        "        new_textA = textA\n",
        "        check = 0 #to allign the text in correct location after first insert\n",
        "        for char_idx in range(len(textA)):\n",
        "            if char_idx == int(row['A-offset']):\n",
        "                new_textA = new_textA[:char_idx+check] + \"person_loc \" + new_textA[char_idx+check:]\n",
        "                check = 11\n",
        "            if char_idx == int(row['Pronoun-offset']):\n",
        "                new_textA = new_textA[:char_idx+check] + \"pronom_loc \" + new_textA[char_idx+check:]\n",
        "                check = 11\n",
        "        testA_X.append(new_textA)\n",
        "        textB = text.lower()\n",
        "        new_textB = textB\n",
        "        check = 0 #to allign the text in correct location after first insert\n",
        "        for char_idx in range(len(textB)):\n",
        "            if char_idx == int(row['B-offset']):\n",
        "                new_textB = new_textB[:char_idx+check] + \"person_loc \" + new_textB[char_idx+check:]\n",
        "                check = 11\n",
        "            if char_idx == int(row['Pronoun-offset']):\n",
        "                new_textB = new_textB[:char_idx+check] + \"pronom_loc \" + new_textB[char_idx+check:]\n",
        "                check = 11\n",
        "        testB_X.append(new_textB)\n",
        "        test_ids.append(row['ID'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_Ov5j4slsIg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "69e93953-c1f3-4eb9-f84c-7912067b1220"
      },
      "cell_type": "code",
      "source": [
        "print(len(test_ids))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YaGUIpRsltZo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########=============================== Load gap-development.tsv ===================\n",
        "with open('/content/gap-coreference/gap-development.tsv') as tsvfile:\n",
        "    reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
        "#     print(*reader)\n",
        "    train_X = []\n",
        "    train_y = []\n",
        "    for row in reader:\n",
        "#         print(row)\n",
        "        text = row[\"Text\"]\n",
        "        text = text.lower()\n",
        "        train_auX.append(text)\n",
        "        new_textA = text\n",
        "        labelA = 0\n",
        "        labelB = 0\n",
        "        check = 0 #to allign the text in correct location after first insert\n",
        "        for char_idx in range(len(text)):\n",
        "            if char_idx == int(row['A-offset']):\n",
        "                new_textA = new_textA[:char_idx+check] + \"person_loc \" + new_textA[char_idx+check:]\n",
        "                check = 11\n",
        "            if char_idx == int(row['Pronoun-offset']):\n",
        "                new_textA = new_textA[:char_idx+check] + \"pronom_loc \" + new_textA[char_idx+check:]\n",
        "                check = 11\n",
        "        if row['A-coref'] == 'TRUE':\n",
        "            labelA = 1\n",
        "        train_X.append(new_textA)\n",
        "        train_y.append(labelA)\n",
        "        new_textB = text\n",
        "        label = 0\n",
        "        check = 0 #to allign the text in correct location after first insert\n",
        "        for char_idx in range(len(text)):\n",
        "            if char_idx == int(row['B-offset']):\n",
        "                new_textB = new_textB[:char_idx+check] + \"person_loc \" + new_textB[char_idx+check:]\n",
        "                check = 11\n",
        "            if char_idx == int(row['Pronoun-offset']):\n",
        "                new_textB = new_textB[:char_idx+check] + \"pronom_loc \" + new_textB[char_idx+check:]\n",
        "                check = 11\n",
        "        if row['B-coref'] == 'TRUE':\n",
        "            labelB = 1\n",
        "        train_X.append(new_textB)\n",
        "        train_y.append(labelB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xzHyV88Wl58u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e8edca4d-4833-468b-979d-eb580f5870a8"
      },
      "cell_type": "code",
      "source": [
        "print(len(train_X))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vGjAFoiqpKM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###============= Tokenize \n",
        "maxlen = 120\n",
        "embed_size = 300\n",
        "max_features = 5000\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "62w66gGmpPia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a3c03f4a-a421-43ab-d408-eac9cf8b0e16"
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer_list = list(train_X)\n",
        "tokenizer.fit_on_texts(tokenizer_list)\n",
        "train_X = tokenizer.texts_to_sequences(train_X)\n",
        "testA_X = tokenizer.texts_to_sequences(testA_X)\n",
        "testB_X = tokenizer.texts_to_sequences(testB_X)\n",
        "test_auX = tokenizer.texts_to_sequences(test_auX)\n",
        "train_auX = tokenizer.texts_to_sequences(train_auX)\n",
        "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "testA_X = pad_sequences(testA_X, maxlen=maxlen)\n",
        "testB_X = pad_sequences(testB_X, maxlen=maxlen)\n",
        "test_auX = pad_sequences(test_auX, maxlen=maxlen)\n",
        "train_auX = pad_sequences(train_auX, maxlen=maxlen)\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "max_features = len(word_index)\n",
        "\n",
        "print(max_features)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mKyYwJgxpfHj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "\n",
        "      \n",
        "      \n",
        "class AttentionWithContext(Layer):\n",
        "    \"\"\"\n",
        "    Attention operation, with a context/query vector, for temporal data.\n",
        "    Supports Masking.\n",
        "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
        "    \"Hierarchical Attention Networks for Document Classification\"\n",
        "    by using a context vector to assist the attention\n",
        "    # Input shape\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(samples, features)`.\n",
        "    How to use:\n",
        "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "    The dimensions are inferred based on the output shape of the RNN.\n",
        "    Note: The layer has been tested with Keras 2.0.6\n",
        "    Example:\n",
        "        model.add(LSTM(64, return_sequences=True))\n",
        "        model.add(AttentionWithContext())\n",
        "        # next add a Dense layer (for classification/regression) or whatever...\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        self.u = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        uit = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            uit += self.b\n",
        "\n",
        "        uit = K.tanh(uit)\n",
        "        ait = dot_product(uit, self.u)\n",
        "\n",
        "        a = K.exp(ait)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZQOpj56prTL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_y = np.asarray(train_y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "142jGoYkpvny",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##==========================Define CuDNNGRU model\n",
        "def get_model():\n",
        "  inp1 = Input(shape=(maxlen,))\n",
        "  inp2 = Input(shape=(maxlen,))\n",
        "\n",
        "  model1_out = Embedding(max_features, embed_size)(inp1)\n",
        "  model1_out = Bidirectional(CuDNNGRU(256, return_sequences=True))(model1_out)\n",
        "  model1_out = AttentionWithContext()(model1_out)\n",
        "  model1_out = Dropout(0.1)(model1_out)\n",
        "\n",
        "  model2_out = Embedding(max_features, embed_size)(inp2)\n",
        "  model2_out = Bidirectional(CuDNNGRU(256, return_sequences=True))(model2_out)\n",
        "  model2_out = AttentionWithContext()(model2_out)\n",
        "  model2_out = Dropout(0.1)(model2_out)\n",
        "\n",
        "  merged_out = keras.layers.Concatenate(axis=1)([model1_out, model2_out])\n",
        "\n",
        "  merged_out = Dense(32, activation=\"relu\")(merged_out)\n",
        "  merged_out = Dropout(0.1)(merged_out)\n",
        "  \n",
        "  merged_out = Dense(1, activation=\"sigmoid\")(merged_out)\n",
        "  model = Model(inputs=[inp1,inp2], outputs=merged_out)\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  \n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e6a6eWRaBcoH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##==========================Define LSTM model\n",
        "def get_model_1():\n",
        "  inp1 = Input(shape=(maxlen,))\n",
        "  inp2 = Input(shape=(maxlen,))\n",
        "\n",
        "  model1_out = Embedding(max_features, embed_size)(inp1)\n",
        "  model1_out = Bidirectional(LSTM(256, return_sequences=True))(model1_out)\n",
        "  model1_out = AttentionWithContext()(model1_out)\n",
        "  model1_out = Dropout(0.1)(model1_out)\n",
        "\n",
        "  model2_out = Embedding(max_features, embed_size)(inp2)\n",
        "  model2_out = Bidirectional(LSTM(256, return_sequences=True))(model2_out)\n",
        "  model2_out = AttentionWithContext()(model2_out)\n",
        "  model2_out = Dropout(0.1)(model2_out)\n",
        "\n",
        "  merged_out = keras.layers.Concatenate(axis=1)([model1_out, model2_out])\n",
        "\n",
        "  merged_out = Dense(32, activation=\"relu\")(merged_out)\n",
        "  merged_out = Dropout(0.1)(merged_out)\n",
        "  \n",
        "  merged_out = Dense(1, activation=\"sigmoid\")(merged_out)\n",
        "  model = Model(inputs=[inp1,inp2], outputs=merged_out)\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  \n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ifhlKUYBtmhB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model_gru():\n",
        "    inp = Input(shape=(maxlen, ))\n",
        "    x = Embedding(max_features, embed_size)(inp)\n",
        "    x = SpatialDropout1D(0.2)(x) # 0.2 -> 0.1\n",
        "    x = Bidirectional(GRU(80, return_sequences=True, activation='relu', dropout=0.1, recurrent_dropout=0.0))(x) # 80 -> 85\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    conc = concatenate([avg_pool, max_pool])\n",
        "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
        "    \n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xfz7MVMytVwE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "62a7b862-7039-4a56-e829-08e2dc685d00"
      },
      "cell_type": "code",
      "source": [
        "model = get_model_1()\n",
        "# model = get_model_gru()\n",
        "print(model.summary())"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            (None, 120)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_10 (InputLayer)           (None, 120)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, 120, 300)     6672900     input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_8 (Embedding)         (None, 120, 300)     6672900     input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_7 (Bidirectional) (None, 120, 512)     1140736     embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_8 (Bidirectional) (None, 120, 512)     1140736     embedding_8[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "attention_with_context_7 (Atten (None, 512)          263168      bidirectional_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "attention_with_context_8 (Atten (None, 512)          263168      bidirectional_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 512)          0           attention_with_context_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 512)          0           attention_with_context_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 1024)         0           dropout_7[0][0]                  \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 32)           32800       concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 32)           0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            33          dropout_9[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 16,186,441\n",
            "Trainable params: 16,186,441\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3vlc3awwp5vJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3673
        },
        "outputId": "91c5b197-4c30-4f95-c3af-9e6bfb02d980"
      },
      "cell_type": "code",
      "source": [
        "model.fit([train_X, train_auX], train_y, batch_size=512, epochs=100, validation_data=([train_X,train_auX], train_y))\n",
        "\n",
        "\n",
        "# model.fit(train_X, train_y, epochs=20, batch_size=32, validation_split=0.1)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/100\n",
            "4000/4000 [==============================] - 22s 5ms/step - loss: 0.6897 - acc: 0.5480 - val_loss: 0.6885 - val_acc: 0.5503\n",
            "Epoch 2/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.6881 - acc: 0.5502 - val_loss: 0.6857 - val_acc: 0.5503\n",
            "Epoch 3/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.6843 - acc: 0.5503 - val_loss: 0.6718 - val_acc: 0.5503\n",
            "Epoch 4/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.6713 - acc: 0.6185 - val_loss: 0.6371 - val_acc: 0.7155\n",
            "Epoch 5/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.6183 - acc: 0.6945 - val_loss: 0.5946 - val_acc: 0.6778\n",
            "Epoch 6/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.5771 - acc: 0.6987 - val_loss: 0.5182 - val_acc: 0.7395\n",
            "Epoch 7/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.5321 - acc: 0.7275 - val_loss: 0.4780 - val_acc: 0.7477\n",
            "Epoch 8/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.4927 - acc: 0.7298 - val_loss: 0.4415 - val_acc: 0.7628\n",
            "Epoch 9/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.4585 - acc: 0.7408 - val_loss: 0.4342 - val_acc: 0.7693\n",
            "Epoch 10/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.4431 - acc: 0.7388 - val_loss: 0.4004 - val_acc: 0.7900\n",
            "Epoch 11/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.4292 - acc: 0.7493 - val_loss: 0.4033 - val_acc: 0.7968\n",
            "Epoch 12/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.4139 - acc: 0.7705 - val_loss: 0.3723 - val_acc: 0.7962\n",
            "Epoch 13/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.3841 - acc: 0.7785 - val_loss: 0.3712 - val_acc: 0.7975\n",
            "Epoch 14/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.3747 - acc: 0.7835 - val_loss: 0.3420 - val_acc: 0.8045\n",
            "Epoch 15/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.3415 - acc: 0.7935 - val_loss: 0.3160 - val_acc: 0.8133\n",
            "Epoch 16/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.3147 - acc: 0.8095 - val_loss: 0.2667 - val_acc: 0.8395\n",
            "Epoch 17/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.2746 - acc: 0.8350 - val_loss: 0.2512 - val_acc: 0.8470\n",
            "Epoch 18/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.2473 - acc: 0.8410 - val_loss: 0.2298 - val_acc: 0.8575\n",
            "Epoch 19/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.2305 - acc: 0.8652 - val_loss: 0.2010 - val_acc: 0.8827\n",
            "Epoch 20/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.2014 - acc: 0.8900 - val_loss: 0.1789 - val_acc: 0.9225\n",
            "Epoch 21/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1839 - acc: 0.9150 - val_loss: 0.1550 - val_acc: 0.9215\n",
            "Epoch 22/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1793 - acc: 0.9275 - val_loss: 0.1896 - val_acc: 0.9347\n",
            "Epoch 23/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1947 - acc: 0.9210 - val_loss: 0.1520 - val_acc: 0.9373\n",
            "Epoch 24/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1745 - acc: 0.9227 - val_loss: 0.1413 - val_acc: 0.9555\n",
            "Epoch 25/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1579 - acc: 0.9438 - val_loss: 0.1322 - val_acc: 0.9713\n",
            "Epoch 26/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1395 - acc: 0.9562 - val_loss: 0.1222 - val_acc: 0.9748\n",
            "Epoch 27/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1335 - acc: 0.9617 - val_loss: 0.1144 - val_acc: 0.9798\n",
            "Epoch 28/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1295 - acc: 0.9635 - val_loss: 0.1065 - val_acc: 0.9777\n",
            "Epoch 29/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1211 - acc: 0.9680 - val_loss: 0.1100 - val_acc: 0.9688\n",
            "Epoch 30/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1159 - acc: 0.9697 - val_loss: 0.1005 - val_acc: 0.9815\n",
            "Epoch 31/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1096 - acc: 0.9713 - val_loss: 0.0956 - val_acc: 0.9865\n",
            "Epoch 32/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1071 - acc: 0.9740 - val_loss: 0.0955 - val_acc: 0.9840\n",
            "Epoch 33/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1075 - acc: 0.9735 - val_loss: 0.0918 - val_acc: 0.9860\n",
            "Epoch 34/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1074 - acc: 0.9703 - val_loss: 0.0922 - val_acc: 0.9860\n",
            "Epoch 35/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1035 - acc: 0.9765 - val_loss: 0.0902 - val_acc: 0.9867\n",
            "Epoch 36/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0998 - acc: 0.9792 - val_loss: 0.0867 - val_acc: 0.9893\n",
            "Epoch 37/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1032 - acc: 0.9720 - val_loss: 0.0857 - val_acc: 0.9890\n",
            "Epoch 38/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0975 - acc: 0.9788 - val_loss: 0.0848 - val_acc: 0.9865\n",
            "Epoch 39/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0975 - acc: 0.9758 - val_loss: 0.0878 - val_acc: 0.9888\n",
            "Epoch 40/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0969 - acc: 0.9790 - val_loss: 0.0888 - val_acc: 0.9900\n",
            "Epoch 41/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0943 - acc: 0.9790 - val_loss: 0.0834 - val_acc: 0.9903\n",
            "Epoch 42/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0920 - acc: 0.9812 - val_loss: 0.0821 - val_acc: 0.9893\n",
            "Epoch 43/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0959 - acc: 0.9750 - val_loss: 0.0806 - val_acc: 0.9912\n",
            "Epoch 44/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0961 - acc: 0.9762 - val_loss: 0.0800 - val_acc: 0.9888\n",
            "Epoch 45/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0915 - acc: 0.9788 - val_loss: 0.0798 - val_acc: 0.9905\n",
            "Epoch 46/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0915 - acc: 0.9803 - val_loss: 0.0827 - val_acc: 0.9895\n",
            "Epoch 47/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0961 - acc: 0.9813 - val_loss: 0.0868 - val_acc: 0.9898\n",
            "Epoch 48/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0972 - acc: 0.9738 - val_loss: 0.0799 - val_acc: 0.9913\n",
            "Epoch 49/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0925 - acc: 0.9812 - val_loss: 0.0786 - val_acc: 0.9915\n",
            "Epoch 50/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0868 - acc: 0.9828 - val_loss: 0.0760 - val_acc: 0.9918\n",
            "Epoch 51/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0837 - acc: 0.9830 - val_loss: 0.0753 - val_acc: 0.9920\n",
            "Epoch 52/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0907 - acc: 0.9775 - val_loss: 0.0786 - val_acc: 0.9915\n",
            "Epoch 53/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0884 - acc: 0.9808 - val_loss: 0.0734 - val_acc: 0.9917\n",
            "Epoch 54/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0869 - acc: 0.9808 - val_loss: 0.0717 - val_acc: 0.9933\n",
            "Epoch 55/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0874 - acc: 0.9805 - val_loss: 0.0738 - val_acc: 0.9905\n",
            "Epoch 56/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0842 - acc: 0.9795 - val_loss: 0.0718 - val_acc: 0.9925\n",
            "Epoch 57/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0868 - acc: 0.9783 - val_loss: 0.0726 - val_acc: 0.9923\n",
            "Epoch 58/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0832 - acc: 0.9818 - val_loss: 0.0741 - val_acc: 0.9920\n",
            "Epoch 59/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0842 - acc: 0.9823 - val_loss: 0.0705 - val_acc: 0.9925\n",
            "Epoch 60/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0830 - acc: 0.9805 - val_loss: 0.0695 - val_acc: 0.9938\n",
            "Epoch 61/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0811 - acc: 0.9835 - val_loss: 0.0680 - val_acc: 0.9942\n",
            "Epoch 62/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0791 - acc: 0.9840 - val_loss: 0.0699 - val_acc: 0.9918\n",
            "Epoch 63/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0793 - acc: 0.9833 - val_loss: 0.0682 - val_acc: 0.9940\n",
            "Epoch 64/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0822 - acc: 0.9780 - val_loss: 0.0701 - val_acc: 0.9880\n",
            "Epoch 65/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0790 - acc: 0.9812 - val_loss: 0.0674 - val_acc: 0.9922\n",
            "Epoch 66/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0782 - acc: 0.9818 - val_loss: 0.0695 - val_acc: 0.9915\n",
            "Epoch 67/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0799 - acc: 0.9815 - val_loss: 0.0666 - val_acc: 0.9930\n",
            "Epoch 68/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0779 - acc: 0.9838 - val_loss: 0.0668 - val_acc: 0.9943\n",
            "Epoch 69/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0746 - acc: 0.9842 - val_loss: 0.0640 - val_acc: 0.9950\n",
            "Epoch 70/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0728 - acc: 0.9868 - val_loss: 0.0630 - val_acc: 0.9947\n",
            "Epoch 71/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0742 - acc: 0.9832 - val_loss: 0.0619 - val_acc: 0.9965\n",
            "Epoch 72/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0719 - acc: 0.9842 - val_loss: 0.0605 - val_acc: 0.9967\n",
            "Epoch 73/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0723 - acc: 0.9833 - val_loss: 0.0589 - val_acc: 0.9977\n",
            "Epoch 74/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0676 - acc: 0.9887 - val_loss: 0.0580 - val_acc: 0.9980\n",
            "Epoch 75/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0673 - acc: 0.9880 - val_loss: 0.0574 - val_acc: 0.9977\n",
            "Epoch 76/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0663 - acc: 0.9888 - val_loss: 0.0564 - val_acc: 0.9977\n",
            "Epoch 77/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0682 - acc: 0.9857 - val_loss: 0.0578 - val_acc: 0.9972\n",
            "Epoch 78/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0654 - acc: 0.9900 - val_loss: 0.0569 - val_acc: 0.9972\n",
            "Epoch 79/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0715 - acc: 0.9860 - val_loss: 0.0632 - val_acc: 0.9950\n",
            "Epoch 80/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0730 - acc: 0.9822 - val_loss: 0.0583 - val_acc: 0.9967\n",
            "Epoch 81/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0733 - acc: 0.9840 - val_loss: 0.0571 - val_acc: 0.9980\n",
            "Epoch 82/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0714 - acc: 0.9845 - val_loss: 0.0590 - val_acc: 0.9967\n",
            "Epoch 83/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0733 - acc: 0.9830 - val_loss: 0.0576 - val_acc: 0.9965\n",
            "Epoch 84/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0711 - acc: 0.9860 - val_loss: 0.0594 - val_acc: 0.9970\n",
            "Epoch 85/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0740 - acc: 0.9833 - val_loss: 0.0580 - val_acc: 0.9965\n",
            "Epoch 86/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0681 - acc: 0.9857 - val_loss: 0.0559 - val_acc: 0.9972\n",
            "Epoch 87/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0658 - acc: 0.9863 - val_loss: 0.0540 - val_acc: 0.9982\n",
            "Epoch 88/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0660 - acc: 0.9867 - val_loss: 0.0536 - val_acc: 0.9985\n",
            "Epoch 89/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0654 - acc: 0.9875 - val_loss: 0.0524 - val_acc: 0.9987\n",
            "Epoch 90/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0640 - acc: 0.9875 - val_loss: 0.0519 - val_acc: 0.9990\n",
            "Epoch 91/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0593 - acc: 0.9913 - val_loss: 0.0510 - val_acc: 0.9992\n",
            "Epoch 92/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0611 - acc: 0.9900 - val_loss: 0.0617 - val_acc: 0.9935\n",
            "Epoch 93/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0650 - acc: 0.9898 - val_loss: 0.0565 - val_acc: 0.9972\n",
            "Epoch 94/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0726 - acc: 0.9833 - val_loss: 0.0862 - val_acc: 0.9770\n",
            "Epoch 95/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0909 - acc: 0.9753 - val_loss: 0.1039 - val_acc: 0.9745\n",
            "Epoch 96/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1025 - acc: 0.9755 - val_loss: 0.1241 - val_acc: 0.9755\n",
            "Epoch 97/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.1137 - acc: 0.9682 - val_loss: 0.0788 - val_acc: 0.9850\n",
            "Epoch 98/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0956 - acc: 0.9732 - val_loss: 0.0659 - val_acc: 0.9942\n",
            "Epoch 99/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0929 - acc: 0.9795 - val_loss: 0.0627 - val_acc: 0.9943\n",
            "Epoch 100/100\n",
            "4000/4000 [==============================] - 17s 4ms/step - loss: 0.0792 - acc: 0.9830 - val_loss: 0.0615 - val_acc: 0.9952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7878853668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "metadata": {
        "id": "MjZt9hhYp_h7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c63c90ac-22c8-49d2-e60d-89bcea67fcaf"
      },
      "cell_type": "code",
      "source": [
        "pred_valA_y = model.predict([testA_X,test_auX], batch_size=512, verbose=1)\n",
        "pred_valB_y = model.predict([testB_X,test_auX], batch_size=512, verbose=1)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 3s 1ms/step\n",
            "2000/2000 [==============================] - 2s 1ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yZXHWKjvxNzV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fd0a0d43-e08b-4f97-fe98-6f7c2223242b"
      },
      "cell_type": "code",
      "source": [
        "print(len(pred_valB_y), \" \", len(pred_valA_y))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000   2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i0dfiT8nxRO5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "out_df = pd.DataFrame({\"ID\":test_ids})\n",
        "out_df['A'] = [max(float(val),0.33) for val in list(pred_valA_y)]\n",
        "out_df['B'] = [max(float(val),0.33) for val in list(pred_valB_y)]\n",
        "out_df['NEITHER'] = [max((1-float(val)),0.33) for val in list(pred_valA_y)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-bwRBhWkxZ-i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "out_df.to_csv(\"submission_2.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uYrFmuMbxenQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "86d19284-8ac8-4a86-ebfe-0d1d2fb8f22e"
      },
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/My Drive/Kaggle Competition: Gendered Pronoun Resolution '"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " gendered-pronoun-resolution\n",
            "'Gendered Pronoun Resolution: Version-01.ipynb'\n",
            " submission_1.csv\n",
            "'Top 3 NLP Libraries Tutorial( NLTK+spaCy+Gensim).ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DruSyLnUxov-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "out_df.to_csv(\"/content/drive/My Drive/Kaggle Competition: Gendered Pronoun Resolution /submission_2.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uuEZiS8Ax62F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "70b9f478-df22-4a57-b792-50b039ffbbe1"
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c gendered-pronoun-resolution -f submission_2.csv -m \"V-02\""
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "100% 103k/103k [00:03<00:00, 33.4kB/s]\n",
            "Successfully submitted to Gendered Pronoun Resolution"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "77C2wmuuyI-y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}