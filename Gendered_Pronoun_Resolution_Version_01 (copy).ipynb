{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gendered Pronoun Resolution: Version-01.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezwanh001/Kaggle-Competitions/blob/master/Gendered_Pronoun_Resolution_Version_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "OaHnm9HUab0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "18e30736-8673-4008-d64c-7651b562e7a5"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kqeNH_3Raktz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import pprint\n",
        "# import tensorflow as tf\n",
        "\n",
        "# if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "#   print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "# else:\n",
        "#   tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "#   print ('TPU address is', tpu_address)\n",
        "\n",
        "#   with tf.Session(tpu_address) as session:\n",
        "#     devices = session.list_devices()\n",
        "    \n",
        "#   print('TPU devices:')\n",
        "#   pprint.pprint(devices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xWirk_Vga7sj",
        "colab_type": "code",
        "outputId": "aad91aa2-6fdf-4efd-9360-3d56f5cc0a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TnCcxAAmbEUV",
        "colab_type": "code",
        "outputId": "bbb4aad0-165a-4f06-a6dd-02cb2a417c38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "cell_type": "code",
      "source": [
        "# to access kaggle datasets\n",
        "!pip install kaggle\n",
        "\n",
        "# Math operations\n",
        "!pip install numpy==1.15.0\n",
        "\n",
        "#\n",
        "!pip install https://github.com/fchollet/keras/archive/cudnn.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.2)\n",
            "Requirement already satisfied: urllib3<1.23.0,>=1.15 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.11.29)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.0.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: Unidecode>=0.04.16 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.0.23)\n",
            "Requirement already satisfied: numpy==1.15.0 in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
            "Collecting https://github.com/fchollet/keras/archive/cudnn.zip\n",
            "\u001b[31m  HTTP error 404 while getting https://github.com/fchollet/keras/archive/cudnn.zip\u001b[0m\n",
            "\u001b[31m  Could not install requirement https://github.com/fchollet/keras/archive/cudnn.zip because of error 404 Client Error: Not Found for url: https://codeload.github.com/keras-team/keras/zip/cudnn\u001b[0m\n",
            "\u001b[31mCould not install requirement https://github.com/fchollet/keras/archive/cudnn.zip because of HTTP error 404 Client Error: Not Found for url: https://codeload.github.com/keras-team/keras/zip/cudnn for URL https://github.com/fchollet/keras/archive/cudnn.zip\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VAxLPHMpbJlJ",
        "colab_type": "code",
        "outputId": "e3075ee4-03f7-40e4-af46-20495cebe509",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "# Colab's file access feature\n",
        "from google.colab import files\n",
        "\n",
        "# retrieve upload file\n",
        "uploaded = files.upload()\n",
        "\n",
        "#print results\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# Then move kaggle.jason into the folder where the API expects to to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/kaggle/kaggle.json "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-14a5a6a0-d471-4631-b71e-f2c2a2d25742\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-14a5a6a0-d471-4631-b71e-f2c2a2d25742\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 65 bytes\n",
            "chmod: cannot access '/root/kaggle/kaggle.json': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_c-nSdN6bNKM",
        "colab_type": "code",
        "outputId": "771b9e05-dd39-427f-ae0e-badea5653c7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "# list competitions\n",
        "!kaggle competitions list\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "ref                                            deadline             category            reward  teamCount  userHasEntered  \n",
            "---------------------------------------------  -------------------  ---------------  ---------  ---------  --------------  \n",
            "digit-recognizer                               2030-01-01 00:00:00  Getting Started  Knowledge       2499           False  \n",
            "titanic                                        2030-01-01 00:00:00  Getting Started  Knowledge       9876            True  \n",
            "house-prices-advanced-regression-techniques    2030-01-01 00:00:00  Getting Started  Knowledge       4079           False  \n",
            "imagenet-object-localization-challenge         2029-12-31 07:00:00  Research         Knowledge         35           False  \n",
            "competitive-data-science-predict-future-sales  2019-12-31 23:59:00  Playground           Kudos       2384           False  \n",
            "two-sigma-financial-news                       2019-07-15 23:59:00  Featured          $100,000       2927           False  \n",
            "LANL-Earthquake-Prediction                     2019-06-03 23:59:00  Research           $50,000       1278            True  \n",
            "tmdb-box-office-prediction                     2019-05-30 23:59:00  Playground       Knowledge        255           False  \n",
            "dont-overfit-ii                                2019-05-07 23:59:00  Playground            Swag        715           False  \n",
            "gendered-pronoun-resolution                    2019-04-22 23:59:00  Research           $25,000        238            True  \n",
            "santander-customer-transaction-prediction      2019-04-10 23:59:00  Featured           $65,000       2015           False  \n",
            "womens-machine-learning-competition-2019       2019-04-09 23:59:00  Featured           $25,000         83           False  \n",
            "mens-machine-learning-competition-2019         2019-04-08 23:59:00  Featured           $25,000        133           False  \n",
            "histopathologic-cancer-detection               2019-03-30 23:59:00  Playground       Knowledge        697           False  \n",
            "petfinder-adoption-prediction                  2019-03-28 23:59:00  Featured           $25,000       1277           False  \n",
            "vsb-power-line-fault-detection                 2019-03-21 23:59:00  Featured           $25,000       1024           False  \n",
            "microsoft-malware-prediction                   2019-03-13 23:59:00  Research           $25,000       1877           False  \n",
            "humpback-whale-identification                  2019-02-28 23:59:00  Featured           $25,000       2055           False  \n",
            "elo-merchant-category-recommendation           2019-02-26 23:59:00  Featured           $50,000       4105            True  \n",
            "ga-customer-revenue-prediction                 2019-02-21 20:04:00  Featured           $45,000       1100            True  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rQwJfvLnbUp-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "bd451e6f-65e5-460a-e6d9-165548aa74ae"
      },
      "cell_type": "code",
      "source": [
        "#download gendered-pronoun-resolution data\n",
        "!kaggle competitions download -c gendered-pronoun-resolution"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading sample_submission_stage_1.csv to /content\n",
            "  0% 0.00/79.0k [00:00<?, ?B/s]\n",
            "100% 79.0k/79.0k [00:00<00:00, 69.5MB/s]\n",
            "Downloading test_stage_1.tsv.zip to /content\n",
            "  0% 0.00/425k [00:00<?, ?B/s]\n",
            "100% 425k/425k [00:00<00:00, 60.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RS-qcHXgf96P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4c41b384-5d9b-49bf-e863-e27ff8ab0800"
      },
      "cell_type": "code",
      "source": [
        "###=============== Import necessary Libraries ==================\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import string\n",
        "import keras\n",
        "from pandas.io.json import json_normalize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "from math import floor\n",
        "import spacy\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from plotly import tools\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import lightgbm as lgb\n",
        "\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, CuDNNGRU, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, Concatenate, Add, Flatten, CuDNNLSTM\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.engine.topology import Layer\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np # linear algebra\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "pd.options.display.max_columns = 999\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# keras libraries\n",
        "from keras.models import Model, load_model,Sequential\n",
        "from keras.layers import Dense, Input, Dropout,Bidirectional, GRU, Activation, concatenate, Embedding, SpatialDropout1D\n",
        "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D ,GlobalMaxPool1D, GlobalAvgPool1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras import layers\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/vnd.plotly.v1+html": "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>",
            "text/html": [
              "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "viX7O-jtgaVy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "083b1d77-ac03-412e-b0c2-2a278ccd5c8d"
      },
      "cell_type": "code",
      "source": [
        "##======== Load text and tokenize ====================\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'A few days later, Abigail complained that Elizabeth was pinching her and tearing at her bowels')\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_, token.dep_)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A DET DT det\n",
            "few ADJ JJ amod\n",
            "days NOUN NNS npadvmod\n",
            "later ADV RB advmod\n",
            ", PUNCT , punct\n",
            "Abigail PROPN NNP nsubj\n",
            "complained VERB VBD ROOT\n",
            "that ADP IN mark\n",
            "Elizabeth PROPN NNP nsubj\n",
            "was VERB VBD aux\n",
            "pinching VERB VBG ccomp\n",
            "her PRON PRP dobj\n",
            "and CCONJ CC cc\n",
            "tearing VERB VBG conj\n",
            "at ADP IN prep\n",
            "her ADJ PRP$ poss\n",
            "bowels NOUN NNS pobj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "feEuY8aQg3LE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "####======================= Define necessary libraries ========================\n",
        "def word_locate(sentence, location): \n",
        "    count_words = 0\n",
        "    count_chars = 2 #2 is to count for the two spaces in the beginning\n",
        "    for word in sentence.split():\n",
        "        count_words += 1\n",
        "        if location == count_chars:\n",
        "            return word, count_words\n",
        "        count_chars += len(word)\n",
        "        count_chars += 1 #for space\n",
        "        \n",
        "def curr_prev_sentence(sentence, loc):\n",
        "    current_sentence = \"\"\n",
        "    prev_sentence = \"\"\n",
        "    detect = 0\n",
        "    count = 0\n",
        "    for char in sentence:\n",
        "        count += 1\n",
        "        current_sentence += char\n",
        "        if char == \".\" and detect == 0:\n",
        "            prev_sentence = current_sentence \n",
        "            current_sentence = \"\"\n",
        "        if char == \".\" and detect == 1:\n",
        "            return current_sentence, prev_sentence\n",
        "        if count == loc:\n",
        "            detect = 1\n",
        "\n",
        "def find_subject(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"nsubj\":\n",
        "            return token.text\n",
        "    return \"none\"            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k9x3mnHahn0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "0cfe91da-1008-48f1-e4aa-1da68bbacfd5"
      },
      "cell_type": "code",
      "source": [
        "text1 = \"After a few years of almost no work -- although he was a guest star on Lou Grant and Charlie's Angels in the late 1970s, he once summed up the 1970s as 'I cried and did a lot of gardening' -- he was hired in 1979 for his best-known role, self-made millionaire Palmer Cortlandt on ABC's long-running soap opera All My Children. Initially hired for only one year, he remained on contract through 2009. For much of his first decade on the show, Palmer was a ruthless villain, totally possessive of his daughter, Nina and violently threatening his ex-wife Daisy with being attacked by dobermans when she came back from the dead.\"\n",
        "\n",
        "text2 = \"When onlookers expressed doubt, claiming that the Proctor family was well regarded in the community, the girl promptly came out of her trance and told them it was all for 'sport'. On March 29, 1692, Abigail Williams and Mercy Lewis again said they were being tormented by Elizabeth's spectre. A few days later, Abigail complained that Elizabeth was pinching her and tearing at her bowels, and said she saw Elizabeth's spectre as well as John's.\"\n",
        "\n",
        "current, prev = curr_prev_sentence(text2, 360)\n",
        "print(current)\n",
        "\n",
        "candidate = find_subject(current)\n",
        "print(candidate)\n",
        "\n",
        "word, loc = word_locate(text2, 360) \n",
        "print(word, \" \", loc)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " A few days later, Abigail complained that Elizabeth was pinching her and tearing at her bowels, and said she saw Elizabeth's spectre as well as John's.\n",
            "Abigail\n",
            "her   60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bYPmwZvbh4hv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7ff8c2c-d27b-47ba-aca0-ad68de7a99fd"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data  sample_submission_stage_1.csv  test_stage_1.tsv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nzK3Yj7YiVIQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6d07e3c5-68da-40f4-9ef3-bd83a0b4ec6c"
      },
      "cell_type": "code",
      "source": [
        "!unzip test_stage_1.tsv.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  test_stage_1.tsv.zip\n",
            "  inflating: test_stage_1.tsv        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y84y7wFuicdD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "6d9bfbcb-0959-4be7-f3db-270232337caf"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "!git clone https://github.com/google-research-datasets/gap-coreference.git\n",
        "\n",
        "#!wget 'https://github.com/google-research-datasets/gap-coreference/blob/master/gap-development.tsv'\n",
        "\n",
        "#!files.download('https://github.com/google-research-datasets/gap-coreference/blob/master/gap-development.tsv') # then browse, select the files. It's then uploaded\n",
        "\n",
        "# uploaded is now a dict containing \"filename\" -> Content"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gap-coreference'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Total 19 (delta 0), reused 0 (delta 0), pack-reused 19\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OSj7ykj9kmPp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2c520651-ddb5-4e32-ef7b-52a629a9b375"
      },
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t\t sample_data\t\t\ttest_stage_1.tsv\n",
            "gap-coreference  sample_submission_stage_1.csv\ttest_stage_1.tsv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RRXhMeXYlBFv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### ======================== load test_stage_1\n",
        "with open('test_stage_1.tsv') as tsvfile:\n",
        "    reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
        "    testA_X = []\n",
        "    testB_X = []\n",
        "    test_auX = []\n",
        "    test_ids = []\n",
        "    train_auX = [] #we do a small hack and fill up the aux train data twice, like train hopping \n",
        "    for row in reader:\n",
        "        text = row['Text']\n",
        "        textA = text.lower()\n",
        "        test_auX.append(text.lower())#test aux is half size of train aux\n",
        "        train_auX.append(text.lower()) #since test loop is running, why not fill up aux train in meantime \n",
        "        new_textA = textA\n",
        "        check = 0 #to allign the text in correct location after first insert\n",
        "        for char_idx in range(len(textA)):\n",
        "            if char_idx == int(row['A-offset']):\n",
        "                new_textA = new_textA[:char_idx+check] + \"person_loc \" + new_textA[char_idx+check:]\n",
        "                check = 11\n",
        "            if char_idx == int(row['Pronoun-offset']):\n",
        "                new_textA = new_textA[:char_idx+check] + \"pronom_loc \" + new_textA[char_idx+check:]\n",
        "                check = 11\n",
        "        testA_X.append(new_textA)\n",
        "        textB = text.lower()\n",
        "        new_textB = textB\n",
        "        check = 0 #to allign the text in correct location after first insert\n",
        "        for char_idx in range(len(textB)):\n",
        "            if char_idx == int(row['B-offset']):\n",
        "                new_textB = new_textB[:char_idx+check] + \"person_loc \" + new_textB[char_idx+check:]\n",
        "                check = 11\n",
        "            if char_idx == int(row['Pronoun-offset']):\n",
        "                new_textB = new_textB[:char_idx+check] + \"pronom_loc \" + new_textB[char_idx+check:]\n",
        "                check = 11\n",
        "        testB_X.append(new_textB)\n",
        "        test_ids.append(row['ID'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_Ov5j4slsIg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5133d7f-e2c9-4e74-a00b-4e2d6fbc9251"
      },
      "cell_type": "code",
      "source": [
        "print(len(test_ids))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YaGUIpRsltZo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########=============================== Load gap-development.tsv ===================\n",
        "with open('/content/gap-coreference/gap-development.tsv') as tsvfile:\n",
        "    reader = csv.DictReader(tsvfile, dialect='excel-tab')\n",
        "#     print(*reader)\n",
        "    train_X = []\n",
        "    train_y = []\n",
        "    for row in reader:\n",
        "#         print(row)\n",
        "        text = row[\"Text\"]\n",
        "        text = text.lower()\n",
        "        train_auX.append(text)\n",
        "        new_textA = text\n",
        "        labelA = 0\n",
        "        labelB = 0\n",
        "        check = 0 #to allign the text in correct location after first insert\n",
        "        for char_idx in range(len(text)):\n",
        "            if char_idx == int(row['A-offset']):\n",
        "                new_textA = new_textA[:char_idx+check] + \"person_loc \" + new_textA[char_idx+check:]\n",
        "                check = 11\n",
        "            if char_idx == int(row['Pronoun-offset']):\n",
        "                new_textA = new_textA[:char_idx+check] + \"pronom_loc \" + new_textA[char_idx+check:]\n",
        "                check = 11\n",
        "        if row['A-coref'] == 'TRUE':\n",
        "            labelA = 1\n",
        "        train_X.append(new_textA)\n",
        "        train_y.append(labelA)\n",
        "        new_textB = text\n",
        "        label = 0\n",
        "        check = 0 #to allign the text in correct location after first insert\n",
        "        for char_idx in range(len(text)):\n",
        "            if char_idx == int(row['B-offset']):\n",
        "                new_textB = new_textB[:char_idx+check] + \"person_loc \" + new_textB[char_idx+check:]\n",
        "                check = 11\n",
        "            if char_idx == int(row['Pronoun-offset']):\n",
        "                new_textB = new_textB[:char_idx+check] + \"pronom_loc \" + new_textB[char_idx+check:]\n",
        "                check = 11\n",
        "        if row['B-coref'] == 'TRUE':\n",
        "            labelB = 1\n",
        "        train_X.append(new_textB)\n",
        "        train_y.append(labelB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xzHyV88Wl58u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b929d87-0aee-4ed0-df24-75dd1691e556"
      },
      "cell_type": "code",
      "source": [
        "print(len(train_X))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vGjAFoiqpKM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###============= Tokenize \n",
        "maxlen = 120\n",
        "embed_size = 300\n",
        "max_features = 5000\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "62w66gGmpPia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "57653938-ebfb-4d1a-9ef6-bd7530285416"
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer_list = list(train_X)\n",
        "tokenizer.fit_on_texts(tokenizer_list)\n",
        "train_X = tokenizer.texts_to_sequences(train_X)\n",
        "testA_X = tokenizer.texts_to_sequences(testA_X)\n",
        "testB_X = tokenizer.texts_to_sequences(testB_X)\n",
        "test_auX = tokenizer.texts_to_sequences(test_auX)\n",
        "train_auX = tokenizer.texts_to_sequences(train_auX)\n",
        "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "testA_X = pad_sequences(testA_X, maxlen=maxlen)\n",
        "testB_X = pad_sequences(testB_X, maxlen=maxlen)\n",
        "test_auX = pad_sequences(test_auX, maxlen=maxlen)\n",
        "train_auX = pad_sequences(train_auX, maxlen=maxlen)\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "max_features = len(word_index)\n",
        "\n",
        "print(max_features)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mKyYwJgxpfHj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "\n",
        "      \n",
        "      \n",
        "class AttentionWithContext(Layer):\n",
        "    \"\"\"\n",
        "    Attention operation, with a context/query vector, for temporal data.\n",
        "    Supports Masking.\n",
        "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
        "    \"Hierarchical Attention Networks for Document Classification\"\n",
        "    by using a context vector to assist the attention\n",
        "    # Input shape\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(samples, features)`.\n",
        "    How to use:\n",
        "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "    The dimensions are inferred based on the output shape of the RNN.\n",
        "    Note: The layer has been tested with Keras 2.0.6\n",
        "    Example:\n",
        "        model.add(LSTM(64, return_sequences=True))\n",
        "        model.add(AttentionWithContext())\n",
        "        # next add a Dense layer (for classification/regression) or whatever...\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        self.u = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        uit = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            uit += self.b\n",
        "\n",
        "        uit = K.tanh(uit)\n",
        "        ait = dot_product(uit, self.u)\n",
        "\n",
        "        a = K.exp(ait)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZQOpj56prTL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_y = np.asarray(train_y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "142jGoYkpvny",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##==========================Define GRU model\n",
        "def get_model():\n",
        "  inp1 = Input(shape=(maxlen,))\n",
        "  inp2 = Input(shape=(maxlen,))\n",
        "\n",
        "  model1_out = Embedding(max_features, embed_size)(inp1)\n",
        "  model1_out = Bidirectional(CuDNNGRU(256, return_sequences=True))(model1_out)\n",
        "  model1_out = AttentionWithContext()(model1_out)\n",
        "  model1_out = Dropout(0.1)(model1_out)\n",
        "\n",
        "  model2_out = Embedding(max_features, embed_size)(inp2)\n",
        "  model2_out = Bidirectional(CuDNNGRU(256, return_sequences=True))(model2_out)\n",
        "  model2_out = AttentionWithContext()(model2_out)\n",
        "  model2_out = Dropout(0.1)(model2_out)\n",
        "\n",
        "  merged_out = keras.layers.Concatenate(axis=1)([model1_out, model2_out])\n",
        "\n",
        "  merged_out = Dense(32, activation=\"relu\")(merged_out)\n",
        "  merged_out = Dropout(0.1)(merged_out)\n",
        "  merged_out = Dense(1, activation=\"sigmoid\")(merged_out)\n",
        "  model = Model(inputs=[inp1,inp2], outputs=merged_out)\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  \n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ifhlKUYBtmhB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model_gru():\n",
        "    inp = Input(shape=(maxlen, ))\n",
        "    x = Embedding(max_features, embed_size)(inp)\n",
        "    x = SpatialDropout1D(0.2)(x) # 0.2 -> 0.1\n",
        "    x = Bidirectional(GRU(80, return_sequences=True, activation='relu', dropout=0.1, recurrent_dropout=0.0))(x) # 80 -> 85\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    conc = concatenate([avg_pool, max_pool])\n",
        "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
        "    \n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xfz7MVMytVwE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "outputId": "bd226213-86b6-469b-9106-903c31faeef6"
      },
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "# model = get_model_gru()\n",
        "print(model.summary())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 120)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 120)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 120, 300)     6672900     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 120, 300)     6672900     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 120, 512)     857088      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 120, 512)     857088      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "attention_with_context_1 (Atten (None, 512)          263168      bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "attention_with_context_2 (Atten (None, 512)          263168      bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           attention_with_context_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 512)          0           attention_with_context_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1024)         0           dropout_1[0][0]                  \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           32800       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32)           0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            33          dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 15,619,145\n",
            "Trainable params: 15,619,145\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3vlc3awwp5vJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "outputId": "b0f3438c-2853-4377-a4d3-e6cf22315c3c"
      },
      "cell_type": "code",
      "source": [
        "model.fit([train_X, train_auX], train_y, batch_size=512, epochs=20, validation_data=([train_X,train_auX], train_y))\n",
        "\n",
        "\n",
        "# model.fit(train_X, train_y, epochs=20, batch_size=32, validation_split=0.1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 4000 samples, validate on 4000 samples\n",
            "Epoch 1/20\n",
            "4000/4000 [==============================] - 12s 3ms/step - loss: 0.6908 - acc: 0.5450 - val_loss: 0.6900 - val_acc: 0.5503\n",
            "Epoch 2/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.6890 - acc: 0.5502 - val_loss: 0.6868 - val_acc: 0.5503\n",
            "Epoch 3/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.6860 - acc: 0.5502 - val_loss: 0.6832 - val_acc: 0.5503\n",
            "Epoch 4/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.6793 - acc: 0.5515 - val_loss: 0.6601 - val_acc: 0.5683\n",
            "Epoch 5/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.6359 - acc: 0.6272 - val_loss: 0.5695 - val_acc: 0.7095\n",
            "Epoch 6/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.5551 - acc: 0.7025 - val_loss: 0.4881 - val_acc: 0.7483\n",
            "Epoch 7/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.5035 - acc: 0.7323 - val_loss: 0.4267 - val_acc: 0.7963\n",
            "Epoch 8/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.4074 - acc: 0.8030 - val_loss: 0.2911 - val_acc: 0.8887\n",
            "Epoch 9/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.3011 - acc: 0.8780 - val_loss: 0.2253 - val_acc: 0.9145\n",
            "Epoch 10/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.2082 - acc: 0.9273 - val_loss: 0.1451 - val_acc: 0.9547\n",
            "Epoch 11/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.1218 - acc: 0.9605 - val_loss: 0.0709 - val_acc: 0.9827\n",
            "Epoch 12/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.0732 - acc: 0.9762 - val_loss: 0.0401 - val_acc: 0.9895\n",
            "Epoch 13/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.0573 - acc: 0.9812 - val_loss: 0.0284 - val_acc: 0.9915\n",
            "Epoch 14/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.0381 - acc: 0.9885 - val_loss: 0.0249 - val_acc: 0.9935\n",
            "Epoch 15/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.0315 - acc: 0.9905 - val_loss: 0.0431 - val_acc: 0.9830\n",
            "Epoch 16/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.0485 - acc: 0.9815 - val_loss: 0.0492 - val_acc: 0.9810\n",
            "Epoch 17/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.0460 - acc: 0.9845 - val_loss: 0.0588 - val_acc: 0.9805\n",
            "Epoch 18/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.0458 - acc: 0.9825 - val_loss: 0.0219 - val_acc: 0.9945\n",
            "Epoch 19/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.0288 - acc: 0.9925 - val_loss: 0.0265 - val_acc: 0.9932\n",
            "Epoch 20/20\n",
            "4000/4000 [==============================] - 7s 2ms/step - loss: 0.0221 - acc: 0.9927 - val_loss: 0.0099 - val_acc: 0.9980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7888751860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "MjZt9hhYp_h7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5f66eea7-67d5-427d-e936-6c6b152df838"
      },
      "cell_type": "code",
      "source": [
        "pred_valA_y = model.predict([testA_X,test_auX], batch_size=512, verbose=1)\n",
        "pred_valB_y = model.predict([testB_X,test_auX], batch_size=512, verbose=1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 1s 568us/step\n",
            "2000/2000 [==============================] - 1s 428us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yZXHWKjvxNzV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7550b858-2f57-4213-e6c0-f908fadcb963"
      },
      "cell_type": "code",
      "source": [
        "print(len(pred_valB_y), \" \", len(pred_valA_y))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000   2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i0dfiT8nxRO5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "out_df = pd.DataFrame({\"ID\":test_ids})\n",
        "out_df['A'] = [max(float(val),0.33) for val in list(pred_valA_y)]\n",
        "out_df['B'] = [max(float(val),0.33) for val in list(pred_valB_y)]\n",
        "out_df['NEITHER'] = [max((1-float(val)),0.33) for val in list(pred_valA_y)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-bwRBhWkxZ-i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "out_df.to_csv(\"submission_1.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uYrFmuMbxenQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "88b8e428-5f8d-48cb-86cd-7b2102aa66ad"
      },
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/My Drive/Kaggle Competition: Gendered Pronoun Resolution '"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " gendered-pronoun-resolution\n",
            "'Gendered Pronoun Resolution: Version-01.ipynb'\n",
            "'Top 3 NLP Libraries Tutorial( NLTK+spaCy+Gensim).ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DruSyLnUxov-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "out_df.to_csv(\"/content/drive/My Drive/Kaggle Competition: Gendered Pronoun Resolution /submission_1.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uuEZiS8Ax62F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "31f8ac06-c820-42a0-c1b1-bf83088b0f6c"
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c gendered-pronoun-resolution -f submission_1.csv -m \"V-01\""
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "100% 103k/103k [00:01<00:00, 56.2kB/s]\n",
            "Successfully submitted to Gendered Pronoun Resolution"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "77C2wmuuyI-y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}